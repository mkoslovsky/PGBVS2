---
title: "PGBVS Vignette"
author: "Matt Koslovsky and Marina Vannucci"
package: PGBVS
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{An Efficient Bayesian Varying-Coefficient Modeling Approach for Behavioral mHealth Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r , echo = FALSE, warning=FALSE, results='hide',message=FALSE}
# library(RefManageR)
# options(knitr.table.format = 'markdown')
# file.name <- system.file("Bib", "state3.bib", package="RefManageR")
# bib <- ReadBib("/Volumes/Samsung_T5/mkoslovsky_mac/RicePostDoc/Businelle/Writing/state3.bib")
# BibOptions(style = "markdown", bib.style = "numeric",cite.style = "numeric")
library(PGBVS)
```

In this vignette, we demonstrate how to implement the underlying method introduced in *An Efficient Bayesian Varying-Coefficient Modeling Approach for Behavioral mHealth Data* using the *PGBVS* package, which performs Bayesian variable selection for random effects as well as varying-coefficients using spiked Dirichlet process priors for logistic regression models with repeated measures data and Polya-Gamma augmentation for efficient sampling. Additionally, we provide functionality to simulate data for sensitivity and simulation studies as well as to perform posterior inference. 

We begin by generating a data set using the `data_sim()` function available in the *PGBVS* package by running `library(PGBVS)` in the R console after successful installation. By default, the algorithm simulates $P = 10$ fixed effects, $\boldsymbol{x}$, and $D = 10$ random effects, $\boldsymbol{z}$, for *n=100* subjects with *20* observations each. For simplicity, continuous covariates are simulated from a standard multivariate normal distribution. Half of the covariates are allowed to jitter from their base value by adding random noise simulated from a standard normal distribution to mimic time-dependent trajectories. Fixed main effects and linear interaction terms are sampled from $\{0,1.5,-1.5,2,-2\}$ with probrabilities $\{.6,.1,.1,.1,.1\}$, respectively. Fixed non-linear interaction terms are generated following the approach taken in the simulation section of the main manuscript and diagonal elements of $\boldsymbol{K}$ are sampled from $\{0,1.5,2\}$ with probability $\{0.6,0.2,0.2\}$, by default. Note that while not required, the `data_sim()` function can be parameterized to include specific fixed main effects and linear interaction terms by setting `beta_bar` to a vector of corresponding coefficients. To specify fixed non-linear interaction terms, `non_linear` can be set to a vector of indicies representing which non-linear interaction terms should be included in the true model.

For this tutorial, we use a data set similar to the data described in the simulation section of the accompanying manuscript. In the true model, the first 5 smooth functions were defined as:
$$f_1(t_{ij}) = \pi\sin(3\pi t_{ij}) + 1.4t_{ij} - 1.6$$
$$f_2(t_{ij}) = \pi\cos(2\pi t_{ij}) + 1.6$$
$$f_3(t_{ij}) = - \pi t\sin(5\pi t_{ij}) + 1.7t_{ij} - 1.5$$
$$f_4(t_{ij}) =  - 1.5t_{ij} + 1.6$$
$$f_5(t_{ij}) =  - 1.6.$$

```{r, echo = FALSE, warning=FALSE, results='hide',message=FALSE }
  set.seed(1)
  N <- 100
  n_i <- sample( seq(20,40), N, replace = TRUE)
  P <- 15
  D <- 15
  cor <- 0.3
  beta_bar <- c( 1.4, -1.6, 0, 1.6, 1.7, -1.5 , -1.5, 1.6, 0, -1.6, rep( 0, 20) )
  non_linear <- c(1,2,3)
  kappa <- diag( D )
  Gamma <- diag( D )
  u1 <- c( 0, 0, 0, 0, 0 )
  sig1 <- matrix( rep(0.4, length( u1 )^2 ), ncol = length( u1 ) )
  diag( sig1 ) <- 0.75
  set.seed(1)
  zeta <- mvtnorm::rmvnorm( sum(n_i), u1, sigma = sig1 )
  zeta <- cbind( zeta, matrix(0, nrow = sum(n_i), ncol = 10) )

  data <- data_sim( N = N, n_i = n_i, P = P, D = D, cor = cor, beta_bar = beta_bar,
                    non_linear = non_linear, kappa = kappa, Gamma = Gamma, zeta = zeta,
                    seed = 1)
```
```{r, echo = FALSE }
str(data)
```

Contained within each `data_sim` object is a vector of output, $\boldsymbol{Y}$, vector of iterations per individual, $\boldsymbol{n_i}$, matrix of fixed covariates, $\boldsymbol{X}$, matrix of covariates by which the effect of $\boldsymbol{X}$ varies for varying-coefficient terms, $\boldsymbol{U}$, matrix of random covariates, $\boldsymbol{Z}$, vector of true fixed main effect and linear-interaction terms, $\boldsymbol{\bar{\beta}}$, and the true matrices for the random effect components $\boldsymbol{K}$,$\boldsymbol{\Gamma}$, and $\boldsymbol{\zeta}$. 

To analyze these data, we run the core function for our method `bvsPG()` and assign its output to the `test` object. Here, we run the algorithm for 7,500 iterations, thinning by every $10^{th}$ iteration with spiked Dirichlet process priors for fixed (`DP_beta = T`) and random (`DP_kappa = T`) effects. 
```{r, eval = FALSE }
test <- bvsPG( iterations = 7500, DP_beta = T, DP_kappa = T,  thin = 10,  Y = data$Y, n_i = data$n_i, Z = data$Z, X = data$X , U = data$U , seed = 1)
```
```{r, echo = FALSE }
file1 <- system.file("extdata", "VignetteExample.RData", package = "PGBVS")
load( file1 )
```

Here, $\boldsymbol{X}$ and $\boldsymbol{Z}$ should be $\sum_{i = 1}^N n_i \times P$- and $\sum_{i = 1}^N n_i \times D$-dimensional matrices (not including intercept terms). $\boldsymbol{U}$ can be a $\sum_{i = 1}^N n_i \times P$- or $\sum_{i = 1}^N n_i \times 1$-dimensional matrix. If the latter, then the algorithm assumes the same $\boldsymbol{U}$ for all varying-coefficient terms. Additionally using the `fixed_avail` and `random_avail` parameters, users can specify which covariates to force into the model. Also all hyperparamters can be adjusted, but we recommend using the default settings as a starting point. 

After running the model, the `test` object contains a list with MCMC samples for the auxillary parameters from the Polya-Gamma augmentation, $\boldsymbol{W}$, fixed regression coefficients, $\boldsymbol{\beta}$, their corresponding inclusion indicators, $\boldsymbol{\nu}$, random effects, $\boldsymbol{K}$, their corresponding inclusion indicators, $\boldsymbol{\lambda}$, cluster assignments for fixed effects, `cluster`, their corresponding values, `cluster_beta`, cluster assignments for random effects, `cluster_K`, and their corresponding values, `cluster_kappa`, along with the MCMC samples for the nuisance parameters outlined in the main manuscript. Additionally, the remaining items of the list contain information regarding computation time, basis functions generated for $\boldsymbol{U}$, and the data supplied to the model for analysis. 
```{r}
str(test)
```
In this example, we treat the first 3,750 (375 after thinning) iterations as burn-in. To determine which fixed effects were selected in the model, use the `selection()` command. This function can be specified to determine inclusion based on the marginal posterior probabilities of inclusion (MPPI) using a Bayesian false discovery rate (BFDR) threshold, which controls for multiplicity. Here, we use a BFDR of 0.10.  
```{r  }
MPPI_fixed <- rowMeans( test$mcmc$v[ ,376:750 ] )
fixed <- selection( MPPI_fixed, 0.10 ) 
cbind(covariate = which(fixed$bfd_selected), s.function = fixed$group_bfd)
```

In addition to the covariates selected using the BFDR, the `selection()` function automatically determines which covariates were selected using the median model approach $(MPPI > 0.50)$, `fixed$med_selected`. Following the approach described in the main manuscript, regression coefficients are indexed in order by their corresponding smooth function components (i.e., non-linear interaction terms, linear interaction terms, and main effects, respectively). For interpretation, the `selection()` function pairs each selected covariate with their corresponding smooth function. Thus, all three components in the first smooth function (intercept terms) were included in the model. However, for the second smooth function, only the non-linear interaction terms and the main effect were selected. 
Overall, we see that the model correctly selected all of the true fixed effects in the model, according the true smooth functions described above. Selection for random effects can be determined similarly by adjusting the MPPI input to `MPPI_random <- rowMeans( test$mcmc$lambda[ ,376:750 ] )`.  To obtain a plot of the MPPIs for both fixed and random effects run
```{r, fig.width = 3.5, fig.height= 2 }
plot.MPPI( bvsPG.out = test, threshold = 0.5, burnin = 376 )
```

In practice, it is of interest to plot the varying-coefficients. By calling the `plot.PGBVS()` function and providing the MCMC output as well as the index of the smooth function you are interested in plotting, `s.function`, a plot is generated of the estimated smooth function with corresponding 95% pointwise credible intervals. For example, 
```{r, fig.width = 3.5, fig.height= 2 }
plot.PGBVS( bvsPG.out = test, s.function = 1   )
```

generates a plot of the log odds ratio as a function of $\boldsymbol{U}$ for the fixed intercept term. To obtain a plot of the second smooth function, simply run 
```{r }
plot.PGBVS( bvsPG.out = test, s.function = 2  )
```

By additionally setting the `exp` parameter to `TRUE`, the estimated odds ratio over $\boldsymbol{U}$ is generated, with corresponding 95% pointwise credible intervals.

While not specifically designed for spiked Dirichlet processes, we recommend using the `sdols` R package to determine latent clusters of regression coefficients. Note that clustering coefficients only makes sense when the `DP_beta` and/or `DP_kappa` parameters in the main function `bvsPG` are set to `TRUE`. We provide a wrapper function `clustering()` which performs the `salso` algorithm using the lower bound of the variation of information loss as default. However the `clustering()` function can take any loss function accepted by the `salso` function. See the `sdols` package for details. 
```{r, message = FALSE }
beta_cluster_MCMC <- test$mcmc$cluster[ seq(1,45)%%3 != 1, 376:750]
clusters <- clustering( MCMC = t( beta_cluster_MCMC ))
clusters$cluster
```

This function requires the MCMC samples (post burn-in) for the main effect and linear interaction terms' cluster allocation. Note that the vector of clusters corresponds to the linear interaction and main effects for each smooth functions in the full model. For example, the first, third, and tenth main effects were clustered together. If the true clustering assignment is known, this function can also be used to determine clustering performance using the `vi.dist` function in the `mcclust` package, which computes the variation of information distance for clusterings, a measure of distance between two clusterings ranging from $0$ to $\log R$, where $R$ is the number of items to cluster and lower values imply better clustering. In practice, true clusters are typically unknown, but the function can be used for simulation studies.  
```{r, message = FALSE}
true_clusters <- c(1,2,3,1,1,2,1,2,3,1,3,2,1,1,3,rep(1,30))[seq(1,45)%%3 != 1]
clusters <- clustering( MCMC = t( beta_cluster_MCMC ), true_clusters = true_clusters )
clusters$eval_vi
```

Lastly to help assess convergence of the model, users can generate plots of the total number of fixed and random effects selected in the model across MCMC iterations. 
```{r, fig.width = 3.5, fig.height= 2 }
plot.selected( bvsPG.out = test )
```

Here, we can see after a burn-in of 375 iterations the number of fixed and random effects in the model stablizes around 12 and 7 terms, respectively. Additionally, the output can easily be transformed into a format that is readable by the `coda` package in R for further summaries, plotting, and diagnostics. 

